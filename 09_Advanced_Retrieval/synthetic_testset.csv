user_input,reference_contexts,reference,synthesizer_name
What about 2025 is important for job stuff?,"['Introduction\nChatGPT launched in November 2022. By July 2025, 18 billion messages were being sent each week\nby 700 million users, representing around 10% of the global adult population.1 For a new technology,\nthis speed of global diffusion has no precedent (Bick et al., 2024).\nThis paper studies consumer usage of ChatGPT, the first mass-market chatbot and likely the\nlargest.2 ChatGPT is based on a Large Language Model (LLM), a type of Artificial Intelligence (AI)\ndeveloped over the last decade and generally considered to represent an acceleration in AI capabilities.3\nThe sudden growth in LLM abilities and adoption has intensified interest in the effects of artificial\nintelligence on economic growth (Acemoglu, 2024; Korinek and Suh, 2024); employment (Eloundou\net al., 2025); and society (Kulveit et al., 2025). However, despite the rapid adoption of LLMs, there\nis limited public information on how they are used. A number of surveys have measured self-reported\nadoption of LLMs (Bick et al., 2024; Pew Research Center, 2025); however there are reasons to expect\nbias in self-reports (Ling and Imas, 2025), and none of these papers have been able to directly track\nthe quantity or nature of chatbot conversations.\nTwo recent papers do report statistics on chatbot conversations, classified in a variety of ways\n(Handa et al., 2025; Tomlinson et al., 2025). We build on this work in several respects. First, the pool\nof users on ChatGPT is far larger, meaning we expect our data to be a closer approximation to the\naverage chatbot user.4 Second, we use automated classifiers to report on the types of messages that\nusers send using new classification taxonomies relative to the existing literature. Third, we report the\ndiffusion of chatbot use across populations and the growth of different types of usage within cohorts.\nFourth, we use a secure data clean room protocol to analyze aggregated employment and education\ncategories for a sample of our users, lending new insights about differences in the types of messages\nsent by different groups while protecting user privacy.\nOur primary sample is a random selection of messages sent to ChatGPT on consumer plans (Free,\nPlus, Pro) between May 2024 and June 2025.5\nMessages from the user to chatbot are classified\nautomatically using a number of different taxonomies: whether the message is used for paid work,\nthe topic of conversation, and the type of interaction (asking, doing, or expressing), and the O*NET\ntask the user is performing. Each taxonomy is defined in a prompt passed to an LLM, allowing us to\nclassify messages without any human seeing them. We give the text of most prompts in Appendix A\nalong with details about how the prompts were validated in Appendix B.6 The classification pipeline is\nprotected by a series of privacy measures, detailed below, to ensure no leakage of sensitive information\nduring the automated analysis. In a secure data clean room, we relate taxonomies of messages to\naggregated employment and education categories.\nTable 1 shows the growth in total message volume for work and non-work usage. Both types of\n1Reuters (2025), Roth (2025)\n2Bick et al. (2024) report that 28% of US adults used ChatGPT in late 2024, higher than any other chatbot.\n3We use the term LLM loosely here and give more details in the following section.\n4Wiggers (2025) reports estimates that in April 2025 ChatGPT was receiving more than 10 times as many visitors\nas either Claude or Copilot.\n5Our sample includes the three consumer plans (Free, Plus, or Pro). OpenAI also offers a variety of other ChatGPT\nplans (Business fka. Teams, Enterprise, Education), which we do not include in our sample.\n6Our classifiers take into account not just the randomly-selected user message, but also a portion of the preceding\nmessages in that conversation.\n1']","The context explains that by July 2025, 18 billion messages were being sent each week by 700 million users, which is around 10% of the global adult population. It also mentions that the growth and adoption of ChatGPT, based on Large Language Models, have increased interest in how AI affects employment among other areas. The study tracks chatbot usage and message types during this period, highlighting the significance of 2025 for understanding AI's impact on work and society.",single_hop_specific_query_synthesizer
How does the Boston Marathon relate to the activities or interests of a Tech Savvy Educator?,"['Table 1: ChatGPT daily message counts (millions), broken down by likely work-related or non-work-related.\nTotal daily counts are exact measurements of message volume from all consumer plans. Daily counts of work\nand non-work related messages are estimated by classifying a random sample of conversations from that day.\nSampling is done to exclude users who opt-out of sharing their messages for model training, users who self-\nreport their age as under 18, logged-out users, deleted conversations, and accounts which have been deactivated\nor banned (details available in Section 3). Reported values are 7-day averages (to smooth weekly fluctuation)\nending on the 26th of June 2024 and 26th of June 2025.\nmessages have grown continuously, but non-work messages have grown faster and now represent more\nthan 70% of all consumer ChatGPT messages. While most economic analysis of AI has focused on its\nimpact on productivity in paid work, the impact on activity outside of work (home production) is on a\nsimilar scale and possibly larger. The decrease in the share of work-related messages is primarily due to\nchanging usage within each cohort of users rather than a change in the composition of new ChatGPT\nusers. This finding is consistent with Collis and Brynjolfsson (2025), who use choice experiments to\nuncover willingness-to-pay for generative AI and estimate a consumer surplus of at least $97 billion\nin 2024 alone in the US.\nWe next report on a classification of messages using a taxonomy developed at OpenAI for un-\nderstanding product usage (“conversation classifier”). Nearly 80% of all ChatGPT usage falls into\nthree broad categories, which we call Practical Guidance, Seeking Information, and Writing. Practical\nGuidance is the most common use case and includes activities like tutoring and teaching, how-to\nadvice about a variety of topics, and creative ideation.7 Seeking Information includes searching for\ninformation about people, current events, products, and recipes, and appears to be a very close sub-\nstitute for web search. Writing includes the automated production of emails, documents and other\ncommunications, but also editing, critiquing, summarizing, and translating text provided by the user.\nWriting is the most common use case at work, accounting for 40% of work-related messages on average\nin June 2025. About two-thirds of all Writing messages ask ChatGPT to modify user text (editing,\ncritiquing, translating, etc.) rather than creating new text from scratch. About 10% of all messages\nare requests for tutoring or teaching, suggesting that education is a key use case for ChatGPT.\nTwo of our findings stand in contrast to other work. First, we find the share of messages related\nto computer coding is relatively small: only 4.2% of ChatGPT messages are related to computer\nprogramming, compared to 33% of work-related Claude conversations Handa et al. (2025).8 Second, we\nfind the share of messages related to companionship or social-emotional issues is fairly small: only 1.9%\nof ChatGPT messages are on the topic of Relationships and Personal Reflection and 0.4% are related\n7The difference between Practical Guidance and Seeking Information is that the former is highly customized to the\nuser and can be adapted based on conversation and follow-up, whereas the latter is factual information that should be\nthe same for all users. For example, users interested in running might ask ChatGPT for the Boston Marathon qualifying\ntimes by age and gender (Seeking Information), or they might ask for a customized workout plan that matches their\ngoals and current level of fitness (Practical Guidance).\n8Handa et al. (2025) report that 37% of conversations are mapped to a “computer and mathematical” occupation\ncategory, and their Figure 12 shows 30% or more of all imputed tasks are programming or IT-related. We believe the\ndiscrepancy is partly due to the difference in types of users between Claude and ChatGPT, additionally Handa et al.\n(2025) only includes queries that ”possibly involve an occupational task”.\n2']","The provided context does not mention the Boston Marathon or its relation to the activities or interests of a Tech Savvy Educator. Therefore, based solely on the given information, there is no direct connection or relevant details to answer this question.",single_hop_specific_query_synthesizer
How does Deming's work relate to the ways ChatGPT can provide economic value and support decision-making in knowledge-intensive jobs?,"['Doing, and that Asking messages are consistently rated as having higher quality both by a classifier\nthat measures user satisfaction and from direct user feedback.\nHow does ChatGPT provide economic value, and for whom is its value the greatest? We argue that\nChatGPT likely improves worker output by providing decision support, which is especially important in\nknowledge-intensive jobs where better decision-making increases productivity (Deming, 2021; Caplin et\nal., 2023). This explains why Asking is relatively more common for educated users who are employed\nin highly-paid, professional occupations.\nOur findings are most consistent with Ide and Talamas\n(2025), who develop a model where AI agents can serve either as co-workers that produce output or\nas co-pilots that give advice and improve the productivity of human problem-solving.\n2\n ']","The context mentions that ChatGPT likely improves worker output by providing decision support, which is especially important in knowledge-intensive jobs where better decision-making increases productivity. It also references Deming (2021) in relation to decision support and productivity, indicating that Deming's work is connected to understanding how AI tools like ChatGPT can enhance decision-making and economic value for professionals in highly-paid, skilled occupations.",single_hop_specific_query_synthesizer
How does GPT-4 enhance digital engagement and educational strategies?,"['How does ChatGPT provide economic value, and for whom is its value the greatest? We argue that\nChatGPT likely improves worker output by providing decision support, which is especially important in\nknowledge-intensive jobs where better decision-making increases productivity (Deming, 2021; Caplin et\nal., 2023). This explains why Asking is relatively more common for educated users who are employed\nin highly-paid, professional occupations.\nOur findings are most consistent with Ide and Talamas\n(2025), who develop a model where AI agents can serve either as co-workers that produce output or\nas co-pilots that give advice and improve the productivity of human problem-solving.\n2\nWhat is ChatGPT?\nHere we give a simplified overview of LLMs and chatbots. For more precise details, refer to the papers\nand system cards that OpenAI has released with each model e.g., (OpenAI, 2023, 2024a, 2025b). A\nchatbot is a statistical model trained to generate a text response given some text input, so as to\nmaximize the “quality” of that response, where the quality is measured with a variety of metrics.\nIn a prototypical interaction, a user submits a plain-text message (“prompt”) and ChatGPT\nreturns the message (“response”) generated from an underlying LLM. A large set of additional features\nhave been added to ChatGPT—including the possibility for the LLM to search the web or external\ndatabases, and generate images based on text—but the exchange of text-based messages remains the\nmost typical interaction.\nSince its launch ChatGPT has used a variety of different underlying LLMs e.g., GPT-3.5, GPT-4,\nGPT-4o, o1, o3, and GPT-5.12 In addition there are occasional updates to the model’s weights and\nto the model’s system prompt (text instructions sent to the model along with all the queries).\nAn LLM can be thought of as a function from a string of words to a probability distribution over\nthe set of all possible words (more precisely, “tokens,” which very roughly correspond to words13). The\nfunctions are implemented with deep neural nets, typically with a transformer architecture (Vaswani\net al., 2017), parameterized with billions of model “weights”. We will refer to all of ChatGPT’s models\nas language models, though most can additionally process tokens representing images, audio, or other\nmedia.\nThe weights in an LLM-based chatbot are often trained in two stages, commonly called “pre-\ntraining” and “post-training”. In the first stage (“pre-training”), the LLMs are trained to predict the\nnext word in a string, given the preceding words, over an enormous corpus of text. At that point the\nmodels are purely predictors of the likelihood of the next word given a prior context, and as such they\nhave a relatively narrow application. In the second stage (“post-training”), the models are trained to\nproduce words that comprise “good” responses to some prompt. This stage often consists of a variety\nof different strategies: fine-tuning on a dataset of queries and ideal responses, reinforcement learning\nagainst another model that is trained to grade the quality of a response (Ouyang et al., 2022), or\nreinforcement learning against a function that knows the true response to queries (OpenAI (2024b),\n12For a timeline of model launches, see Appendix C.\n13Tokenization is a way of cutting a string of text into discrete chunks, chosen to be statistically efficient. In many\ntokenization schemes, one token corresponds to roughly three-quarters of an English word.\n4']","The provided context does not include specific information about GPT-4's role in enhancing digital engagement or educational strategies. It primarily explains what ChatGPT is, how it functions, and details about large language models and their training processes.",single_hop_specific_query_synthesizer
Are Latin American users more likely to use ChatGPT for occupational advice?,"['6\n Who Uses ChatGPT\nIn this section we report basic descriptive facts about who uses consumer ChatGPT. Existing work\ndocuments variation in generative AI use by demographic groups within representative samples in\nthe U.S. (Bick et al. (2024), Hartley et al. (2025)) and within a subset of occupations in Denmark\n(Humlum and Vestergaard, 2025a). All of these papers find that generative AI is used more frequently\nby men, young people, and those with tertiary and/or graduate education.\nWe make three contributions relative to this prior literature. First, we confirm these broad demo-\ngraphic patterns in a global sample rather than a single country. Second, we provide more detail for\nselected demographics such as age, gender, and country of origin and study how gaps in each have\nchanged over time. Third, we use a secure data clean room to analyze how ChatGPT usage varies by\neducation and occupation.\n6.1\n Name Analysis\nWe investigate potential variation by gender by classifying a global random sample of over 1.1 million\nChatGPT users’ first names using public aggregated datasets of name-gender associations. We used\nthe World Gender Name Dictionary, and Social Security popular names, as well as datasets of popular\nBrazilian and Latin American names. This methodology is similar to that in (Hofstra et al., 2020)\nand (West et al., 2013). Names that were not in these datasets, or were flagged as ambiguous in the\ndatasets, or had significant disagreement amongst these datasets were classified as Unknown.\nExcluding Unknown, a significant share (around 80%) of the weekly active users (WAU) in the\nfirst few months after ChatGPT was released were by users with typically masculine first names.\nHowever, in the first half of 2025, we see the share of active users with typically feminine and typically\nmasculine names reach near-parity. By June 2025 we observe active users are more likely to have\ntypically feminine names. This suggests that gender gaps in ChatGPT usage have closed substantially\nover time.\nWe also study differences in usage topics. Users with typically female first names are relatively more\nlikely to send messages related to Writing and Practical Guidance. By contrast, users with typically\nmale first names are more likely to use ChatGPT for Technical Help, Seeking Out Information, and\nMultimedia (e.g., modifying or creating images).\n6.2\n']","The provided context does not include specific information about Latin American users' purposes for using ChatGPT or their occupational interests. It mainly discusses demographic patterns, name classifications, and usage trends over time, but does not specify how Latin American users utilize ChatGPT in relation to occupational advice.",single_hop_specific_query_synthesizer
What ChatGPT do for students?,"['Variation by Age\nA subset of users self-report their age when registering for OpenAI. Among those who self-report their\nage, around 46% of the messages in our dataset are accounted for by users 18-25.\nA higher share of messages are work-related for older users. Work-related messages comprised\napproximately 23% of messages for users under age 26, with this share increasing with age. The\none exception is users who self-attest to being 66 years-old or older, with only 16% of their classified\nmessages being work-related. The plot below shows trends in the share of work-related messages by\nage group. ChatGPT usage has become less work-related over time for users of all ages.\n25']","ChatGPT is used by users of different ages, with many messages from 18-25 age group. Older users tend to use it more for work, but overall, ChatGPT usage has become less work-related over time for all ages.",single_hop_specific_query_synthesizer
How occupation impact ChatGPT work use and is it significant cause of difference in work tasks?,"['<1-hop>\n\nVariation by Occupation\nFigure 23 presents variation in ChatGPT usage by user occupation. Due to privacy-preserving aggre-\ngation limits, we report results for the following broad occupation categories – (1) all nonprofessional\noccupations, including administrative, clerical, service, and blue-collar occupations; (2) computer-\nrelated occupations; (3) engineering and science occupations; (4) management and business occupa-\ntions; and (5) all other professional occupations, including law, education, and health care.26\nAs\nabove, the left-hand side of the figure shows unadjusted comparisons and the right-hand side presents\nthe coefficients on each occupation category from a regression of message shares on age, whether the\nname was typically masculine or feminine, education, occupation categories, job seniority, firm size,\nand industry.\nUsers in highly paid professional and technical occupations are more likely to use ChatGPT for\nwork.27 Panel A shows that the unadjusted work shares are 57% for computer-related occupations;\n50% for management and business; 48% for engineering and science; 44% for other professional oc-\ncupations; and only 40% for all non-professional occupations. Regression adjustment moves these\nfigures around slightly, but the gaps by occupation remain highly statistically significant. Users in\nhighly-paid professional occupations are more likely to send work-related messages.\nBecause work usage is so different by occupation, we restrict the sample only to work-related\nmessages in Panels B and C. Panel B presents the share of work-related messages that are Asking\nmessages, by occupation. We find that users in highly paid professional occupations are more likely\nto use ChatGPT for Asking rather than Doing.28 This is especially true in scientific and technical\noccupations. 47% of the work-related messages sent by users employed in computer-related occupa-\ntions are Asking messages, compared to only 32% for non-professional occupations. These differences\nshrink somewhat with regression adjustment, but remain highly statistically significant.\nPanel C presents results by conversation topic. Writing is especially common for users employed\nin management and business occupations, accounting for 52% of all work-related messages. Writing\nis also relatively common in non-professional and other professional occupations like education and\nhealth care, accounting for 50% and 49% of work-related messages respectively. Technical Help consti-\ntutes 37% of all work-related messages for users employed in computer-related occupations, compared\nto 16% in engineering and science and only about 8% for all other categories. Regression adjustment\naffects gaps by occupation only modestly. Overall there are stark differences in the distribution of\nconversation topics by user occupation, with work-related messages clearly focused on the core tasks\nin each job (e.g. Writing for management and business, Technical Help for technical occupations).\nWe also present data on the most common Generalized Work Activities (GWAs) associated with\neach broad occupation group, as measured by 2-digit Standard Occupation Classification (SOC) codes.\nTable 24 presents the frequency ranking of work-related messages in each SOC code of the seven most\ncommon GWAs.29\n26Management and business are SOC2 codes 11 and 13. Computer-related is SOC2 code 15. Engineering and Science\nare SOC2 codes 17 and 19. Other Professional are SOC2 codes 21 to 29. Nonprofessional occupations are SOC codes\n31 to 53.\n27As discussed in Section: Data and Privacy, our dataset only includes users on ChatGPT Consumer plans. Corporate\nusers may also use ChatGPT Business (formerly known as Teams) or ChatGPT Enterprise.\n28Very few work-related messages are classified as Expressing.\n29Appendix D contains a full report of GWA counts broken down by occupation, for both work-related ChatGPT\n31']","The context shows that different occupations have varying levels of ChatGPT usage for work tasks, with highly paid professional and technical jobs using ChatGPT more often. Regression analysis confirms these differences are statistically significant, indicating occupation impacts ChatGPT usage and the types of work tasks like Asking questions or writing are more common in certain occupations, especially in technical and management fields.",multi_hop_abstract_query_synthesizer
How conversation topics like writing and technical help relate to work activities and SOC codes in different occupations?,"['<1-hop>\n\nVariation by Occupation\nFigure 23 presents variation in ChatGPT usage by user occupation. Due to privacy-preserving aggre-\ngation limits, we report results for the following broad occupation categories – (1) all nonprofessional\noccupations, including administrative, clerical, service, and blue-collar occupations; (2) computer-\nrelated occupations; (3) engineering and science occupations; (4) management and business occupa-\ntions; and (5) all other professional occupations, including law, education, and health care.26\nAs\nabove, the left-hand side of the figure shows unadjusted comparisons and the right-hand side presents\nthe coefficients on each occupation category from a regression of message shares on age, whether the\nname was typically masculine or feminine, education, occupation categories, job seniority, firm size,\nand industry.\nUsers in highly paid professional and technical occupations are more likely to use ChatGPT for\nwork.27 Panel A shows that the unadjusted work shares are 57% for computer-related occupations;\n50% for management and business; 48% for engineering and science; 44% for other professional oc-\ncupations; and only 40% for all non-professional occupations. Regression adjustment moves these\nfigures around slightly, but the gaps by occupation remain highly statistically significant. Users in\nhighly-paid professional occupations are more likely to send work-related messages.\nBecause work usage is so different by occupation, we restrict the sample only to work-related\nmessages in Panels B and C. Panel B presents the share of work-related messages that are Asking\nmessages, by occupation. We find that users in highly paid professional occupations are more likely\nto use ChatGPT for Asking rather than Doing.28 This is especially true in scientific and technical\noccupations. 47% of the work-related messages sent by users employed in computer-related occupa-\ntions are Asking messages, compared to only 32% for non-professional occupations. These differences\nshrink somewhat with regression adjustment, but remain highly statistically significant.\nPanel C presents results by conversation topic. Writing is especially common for users employed\nin management and business occupations, accounting for 52% of all work-related messages. Writing\nis also relatively common in non-professional and other professional occupations like education and\nhealth care, accounting for 50% and 49% of work-related messages respectively. Technical Help consti-\ntutes 37% of all work-related messages for users employed in computer-related occupations, compared\nto 16% in engineering and science and only about 8% for all other categories. Regression adjustment\naffects gaps by occupation only modestly. Overall there are stark differences in the distribution of\nconversation topics by user occupation, with work-related messages clearly focused on the core tasks\nin each job (e.g. Writing for management and business, Technical Help for technical occupations).\nWe also present data on the most common Generalized Work Activities (GWAs) associated with\neach broad occupation group, as measured by 2-digit Standard Occupation Classification (SOC) codes.\nTable 24 presents the frequency ranking of work-related messages in each SOC code of the seven most\ncommon GWAs.29\n26Management and business are SOC2 codes 11 and 13. Computer-related is SOC2 code 15. Engineering and Science\nare SOC2 codes 17 and 19. Other Professional are SOC2 codes 21 to 29. Nonprofessional occupations are SOC codes\n31 to 53.\n27As discussed in Section: Data and Privacy, our dataset only includes users on ChatGPT Consumer plans. Corporate\nusers may also use ChatGPT Business (formerly known as Teams) or ChatGPT Enterprise.\n28Very few work-related messages are classified as Expressing.\n29Appendix D contains a full report of GWA counts broken down by occupation, for both work-related ChatGPT\n31']","Variation by occupation shows that users in management, business, and technical fields use ChatGPT for specific work-related topics such as writing and technical help. For example, 52% of work messages from management and business occupations involve writing, while technical help constitutes 37% for computer-related jobs. The data also links these conversation topics to Standard Occupation Classification (SOC) codes, indicating that work activities like writing and technical assistance are prevalent in certain SOC groups, such as SOC codes 15 for computer-related and 17/19 for engineering and science occupations. This demonstrates how conversation topics like writing and technical help are closely associated with specific work activities and SOC classifications across different occupations.",multi_hop_abstract_query_synthesizer
How do annotation procedures and context handling relate to work activities classified by SOC codes in chatbot analysis?,"[""<1-hop>\n\nAppendix: Classifier Validation\nTo assess the performance of our classifiers, we compare LLM-generated labels to human labels on a\npublicly available corpus of chatbot conversations (WildChat; Zhao et al., 2024). Annotations were\ncarried out by several in-house annotators31.\nTable 5 reports agreement rates both among humans and between the model and human annota-\ntions across all tasks.\nTask\nnlabels\nFleiss’ κ\n(human only)\nFleiss’ κ\n(with model)\nCohen’s κ\n(human vs. human)\nCohen’s κ\n(model vs. plurality)\nWork Related (binary)\n149\n0.66 [0.54, 0.76]\n0.68 [0.59, 0.77]\n0.66\n0.83 [0.72, 0.92]\nAsking / Doing /\nExpressing (3-class)\n149\n0.60 [0.51, 0.68]\n0.63 [0.56, 0.70]\n0.60\n0.74 [0.64, 0.83]\nConversation Topic\n(coarse)\n149\n0.46 [0.38, 0.53]\n0.48 [0.41, 0.54]\n0.47\n0.56 [0.46, 0.65]\nIWA Classification\n100\n0.34 [0.23, 0.45]\n0.47 [0.40, 0.53]\n0.37\n—\nGWA Classification\n100\n0.33 [0.22, 0.44]\n0.47 [0.40, 0.54]\n0.36\n—\nInteraction Quality\n(3-class incl. unknown)\n149\n0.13 [0.04, 0.22]\n0.10 [0.04, 0.17]\n0.20\n0.14 [0.01, 0.27]\nTable 5: Validation topline results. ”—” indicates classifiers where only two human annotators participated\nand a plurality measure was not possible.\nFor each task we report: (i) Fleiss’ κ across human annotators; (ii) Fleiss’κ when treating the\nmodel as an additional annotator; (iii) the mean pairwise human–human Cohen’s κ; and (iv) Cohen’s\nκ between the model and the human plurality label. An item contributes to a statistic only if all\nrequired raters provided a nonempty label. Confidence intervals are 95% percentile intervals (2.5th\nand 97.5th percentiles) from a nonparametric bootstrap with 2,000 resamples.\nTo annotate these messages, we replicate the procedure from Section 3. For each conversation, the\nclassifier is applied to a randomly selected user message along with up to the 10 preceding messages\n(each truncated to 5,000 characters). Because this context can be lengthy, human annotators also\nreceived a one-sentence pr´ecis of the preceding messages, generated using the following prompt:\n-----\nYou are an internal tool that writes a one-sentence precis of a\nmessage from a user to an AI chatbot, based on the context of the\nprevious messages before it. Write a precis of the user intent in the\nlast user message of this conversation, 25 words at most.\nE.g. 'User is rewriting email to neighbors about\nplumbing to be more friendly,'\nor 'User is complaining about grandmother'\nor 'User is asking for help fixing python databricks error.'\n31The IWA classifications were carried out by two annotators, while all other classifications had three.\n52"", '<2-hop>\n\nVariation by Occupation\nFigure 23 presents variation in ChatGPT usage by user occupation. Due to privacy-preserving aggre-\ngation limits, we report results for the following broad occupation categories – (1) all nonprofessional\noccupations, including administrative, clerical, service, and blue-collar occupations; (2) computer-\nrelated occupations; (3) engineering and science occupations; (4) management and business occupa-\ntions; and (5) all other professional occupations, including law, education, and health care.26\nAs\nabove, the left-hand side of the figure shows unadjusted comparisons and the right-hand side presents\nthe coefficients on each occupation category from a regression of message shares on age, whether the\nname was typically masculine or feminine, education, occupation categories, job seniority, firm size,\nand industry.\nUsers in highly paid professional and technical occupations are more likely to use ChatGPT for\nwork.27 Panel A shows that the unadjusted work shares are 57% for computer-related occupations;\n50% for management and business; 48% for engineering and science; 44% for other professional oc-\ncupations; and only 40% for all non-professional occupations. Regression adjustment moves these\nfigures around slightly, but the gaps by occupation remain highly statistically significant. Users in\nhighly-paid professional occupations are more likely to send work-related messages.\nBecause work usage is so different by occupation, we restrict the sample only to work-related\nmessages in Panels B and C. Panel B presents the share of work-related messages that are Asking\nmessages, by occupation. We find that users in highly paid professional occupations are more likely\nto use ChatGPT for Asking rather than Doing.28 This is especially true in scientific and technical\noccupations. 47% of the work-related messages sent by users employed in computer-related occupa-\ntions are Asking messages, compared to only 32% for non-professional occupations. These differences\nshrink somewhat with regression adjustment, but remain highly statistically significant.\nPanel C presents results by conversation topic. Writing is especially common for users employed\nin management and business occupations, accounting for 52% of all work-related messages. Writing\nis also relatively common in non-professional and other professional occupations like education and\nhealth care, accounting for 50% and 49% of work-related messages respectively. Technical Help consti-\ntutes 37% of all work-related messages for users employed in computer-related occupations, compared\nto 16% in engineering and science and only about 8% for all other categories. Regression adjustment\naffects gaps by occupation only modestly. Overall there are stark differences in the distribution of\nconversation topics by user occupation, with work-related messages clearly focused on the core tasks\nin each job (e.g. Writing for management and business, Technical Help for technical occupations).\nWe also present data on the most common Generalized Work Activities (GWAs) associated with\neach broad occupation group, as measured by 2-digit Standard Occupation Classification (SOC) codes.\nTable 24 presents the frequency ranking of work-related messages in each SOC code of the seven most\ncommon GWAs.29\n26Management and business are SOC2 codes 11 and 13. Computer-related is SOC2 code 15. Engineering and Science\nare SOC2 codes 17 and 19. Other Professional are SOC2 codes 21 to 29. Nonprofessional occupations are SOC codes\n31 to 53.\n27As discussed in Section: Data and Privacy, our dataset only includes users on ChatGPT Consumer plans. Corporate\nusers may also use ChatGPT Business (formerly known as Teams) or ChatGPT Enterprise.\n28Very few work-related messages are classified as Expressing.\n29Appendix D contains a full report of GWA counts broken down by occupation, for both work-related ChatGPT\n31']","The annotation procedures involve comparing LLM-generated labels to human labels on chatbot conversations, assessing agreement rates across tasks such as work-related classification. The context handling includes analyzing lengthy conversation data, which is relevant for classifying work activities using Standard Occupation Classification (SOC) codes, as shown in the variation of ChatGPT usage by occupation and work-related message topics. This multi-hop analysis connects the annotation validation process with the classification of work activities based on occupation categories.",multi_hop_abstract_query_synthesizer
"Considering the extensive data on ChatGPT's usage patterns and the reported AI's economic impact and revenue, how do user engagement metrics and prompt usage statistics from recent reports inform our understanding of ChatGPT's role in driving its financial success and evolving educational strategies?","['<1-hop>\n\nOpenAI, “GPT-4 Technical Report,” 2023. arXiv preprint.\n, “GPT-4o System Card,” https://cdn.openai.com/gpt-4o-system-card.pdf 2024.\n, “OpenAI o1 System Card,” System Card / Technical Report, arXiv December 2024. Submitted\n21 December 2024.\n, “Expanding on What We Missed with Sycophancy,” Blog Post / Technical Report, OpenAI May\n2025. A detailed follow-up on the GPT-4o sycophancy rollback, outlining causes and improvements.\n, “GPT-5 System Card,” System Card / Technical Report August 2025.\nGPT-5 system card,\nOpenAI.\n, “Privacy Policy,” https://openai.com/policies/row-privacy-policy/ 2025. last updated June 27,\n2025.\nOuyang, Long, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, Pamela\nMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, John Schul-\nman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, Amanda Askell, Peter\nWelinder, Paul Christiano, Jan Leike, and Ryan Lowe, “Training Language Models to\nFollow Instructions with Human Feedback,” 2022.\nPew Research Center, “U.S. adults’ use of ChatGPT (June 2025 report),” 2025.\nPhang, Jason, Michael Lampe, Lama Ahmad, Sandhini Agarwal, Cathy Mengying Fang,\nAuren R. Liu, Valdemar Danry, Eunhae Lee, Samantha W. T. Chan, Pat Pataranuta-\nporn, and Pattie Maes, “Investigating Affective Use and Emotional Well-being on ChatGPT,”\n2025.\nReuters, “OpenAI hits $12 billion in annualized revenue, The Information reports,” Reuters, July\n30 2025. Accessed: 2025-09-11.\nRoth, Emma, “OpenAI says ChatGPT users send over 2.5 billion prompts every day,” July 21 2025.\nAccessed: 2025-09-11.\nTomlinson, Kiran, Sonia Jaffe, Will Wang, Scott Counts, and Siddharth Suri, “Working\nwith AI: Measuring the Occupational Implications of Generative AI,” 2025.\n39']","The reports indicate that ChatGPT users send over 2.5 billion prompts daily, reflecting high engagement levels. Additionally, OpenAI has achieved over $12 billion in annualized revenue, suggesting that the widespread prompt usage significantly contributes to its economic impact. These user engagement metrics demonstrate the platform's integral role in digital interaction, which can inform educational strategies by highlighting how evolving usage patterns influence learning and engagement with AI tools. The combination of high prompt activity and substantial revenue underscores the importance of understanding user behavior to optimize educational applications and assess AI's broader influence on digital engagement and economic growth.",multi_hop_abstract_query_synthesizer
