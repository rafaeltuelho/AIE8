{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LangGraph Open Deep Research - Supervisor-Researcher Architecture\n",
    "\n",
    "In this notebook, we'll explore the **supervisor-researcher delegation architecture** for conducting deep research with LangGraph.\n",
    "\n",
    "You can visit this repository to see the original application: [Open Deep Research](https://github.com/langchain-ai/open_deep_research)\n",
    "\n",
    "<img style=\"max-width: 65%; height: 50%;\" alt=\"full_diagram\" src=\"https://github.com/user-attachments/assets/12a2371b-8be2-4219-9b48-90503eb43c69\" />\n",
    "\n",
    "Let's jump in!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What We're Building\n",
    "\n",
    "This implementation uses a **hierarchical delegation pattern** where:\n",
    "\n",
    "1. **User Clarification** - Optionally asks clarifying questions to understand the research scope\n",
    "2. **Research Brief Generation** - Transforms user messages into a structured research brief\n",
    "3. **Supervisor** - A lead researcher that analyzes the brief and delegates research tasks\n",
    "4. **Parallel Researchers** - Multiple sub-agents that conduct focused research simultaneously\n",
    "5. **Research Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings are combined into a comprehensive report\n",
    "\n",
    "<!-- img src=\"https://private-user-images.githubusercontent.com/181020547/465825499-052f2ed3-c664-4a4f-8ec2-074349dcaa3f.png?jwt=eyJ0eXAiOiJKV1QiLCJhbGciOiJIUzI1NiJ9.eyJpc3MiOiJnaXRodWIuY29tIiwiYXVkIjoicmF3LmdpdGh1YnVzZXJjb250ZW50LmNvbSIsImtleSI6ImtleTUiLCJleHAiOjE3NjAzODY3NjUsIm5iZiI6MTc2MDM4NjQ2NSwicGF0aCI6Ii8xODEwMjA1NDcvNDY1ODI1NDk5LTA1MmYyZWQzLWM2NjQtNGE0Zi04ZWMyLTA3NDM0OWRjYWEzZi5wbmc_WC1BbXotQWxnb3JpdGhtPUFXUzQtSE1BQy1TSEEyNTYmWC1BbXotQ3JlZGVudGlhbD1BS0lBVkNPRFlMU0E1M1BRSzRaQSUyRjIwMjUxMDEzJTJGdXMtZWFzdC0xJTJGczMlMkZhd3M0X3JlcXVlc3QmWC1BbXotRGF0ZT0yMDI1MTAxM1QyMDE0MjVaJlgtQW16LUV4cGlyZXM9MzAwJlgtQW16LVNpZ25hdHVyZT1kMGMyN2JlYzQ4MjI5OTVhNWQ2ZGE2ZjYyMjg1YmU0YjlhNmIyYjY5ZjRjNTNjOWU3MjBlZjQ0YzE4OWNjODQ0JlgtQW16LVNpZ25lZEhlYWRlcnM9aG9zdCJ9.6Sx4OGFIVxY1281--A4kyfYsA2LedHUA-162lmmqYBQ\" alt=\"Architecture Diagram\" style=\"max-width: 50%; height: 50%;\" / -->\n",
    "\n",
    "<img src=\"./detailed_agent_graph2.png\" alt=\"Architecture Diagram\" style=\"max-width: 50%; height: 50%;\" />\n",
    "\n",
    "\n",
    "This differs from a section-based approach by allowing dynamic task decomposition based on the research question, rather than predefined sections."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies\n",
    "\n",
    "You'll need API keys for Anthropic (for the LLM) and Tavily (for web search). We'll configure the system to use Anthropic's Claude Sonnet 4 exclusively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import getpass\n",
    "\n",
    "os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter your Anthropic API key: \")\n",
    "os.environ[\"TAVILY_API_KEY\"] = getpass.getpass(\"Enter your Tavily API key: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uuid import uuid4\n",
    "# Enable LangSmith tracing\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
    "os.environ[\"LANGCHAIN_API_KEY\"] = getpass.getpass(\"LangChain API Key:\")\n",
    "os.environ[\"LANGCHAIN_PROJECT\"] = f\"AIM - Open Deep Research - {uuid4().hex[0:8]}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: State Definitions\n",
    "\n",
    "The state structure is hierarchical with three levels:\n",
    "\n",
    "### Agent State (Top Level)\n",
    "Contains the overall conversation messages, research brief, accumulated notes, and final report.\n",
    "\n",
    "### Supervisor State (Middle Level)\n",
    "Manages the research supervisor's messages, research iterations, and coordinating parallel researchers.\n",
    "\n",
    "### Researcher State (Bottom Level)\n",
    "Each individual researcher has their own message history, tool call iterations, and research findings.\n",
    "\n",
    "We also have structured outputs for tool calling:\n",
    "- **ConductResearch** - Tool for supervisor to delegate research to a sub-agent\n",
    "- **ResearchComplete** - Tool to signal research phase is done\n",
    "- **ClarifyWithUser** - Structured output for asking clarifying questions\n",
    "- **ResearchQuestion** - Structured output for the research brief\n",
    "\n",
    "Let's import these from our library: [`open_deep_library/state.py`](open_deep_library/state.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import state definitions from the library\n",
    "from open_deep_library.state import (\n",
    "    # Main workflow states\n",
    "    AgentState,           # Lines 65-72: Top-level agent state with messages, research_brief, notes, final_report\n",
    "    AgentInputState,      # Lines 62-63: Input state is just messages\n",
    "    \n",
    "    # Supervisor states\n",
    "    SupervisorState,      # Lines 74-81: Supervisor manages research delegation and iterations\n",
    "    \n",
    "    # Researcher states\n",
    "    ResearcherState,      # Lines 83-90: Individual researcher with messages and tool iterations\n",
    "    ResearcherOutputState, # Lines 92-96: Output from researcher (compressed research + raw notes)\n",
    "    \n",
    "    # Structured outputs for tool calling\n",
    "    ConductResearch,      # Lines 15-19: Tool for delegating research to sub-agents\n",
    "    ResearchComplete,     # Lines 21-22: Tool to signal research completion\n",
    "    ClarifyWithUser,      # Lines 30-41: Structured output for user clarification\n",
    "    ResearchQuestion,     # Lines 43-48: Structured output for research brief\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ❓ Question 1:\n",
    "\n",
    " Explain the interrelationships between the three states.  Why don't we just make a single huge state?\n",
    "\n",
    "##### ✅ Answer:\n",
    "The states are defined in a hierarchical structure:\n",
    " * Agent (Top level)\n",
    " * Supervisor (Middle level)\n",
    " * Researcher (Botton level) \n",
    " \n",
    " Each agent has it own state to manage only the context relevant to its individual task and its respective interactions with the LLM. This approach allow separation of concerns and avoid context polution where actions taken by one agent can confuse other agents in the same level. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Utility Functions and Tools\n",
    "\n",
    "The system uses several key utilities:\n",
    "\n",
    "### Search Tools\n",
    "- **tavily_search** - Async web search with automatic summarization to stay within token limits\n",
    "- Supports Anthropic native web search and Tavily API\n",
    "\n",
    "### Reflection Tools\n",
    "- **think_tool** - Allows researchers to reflect on their progress and plan next steps (ReAct pattern)\n",
    "\n",
    "### Helper Utilities\n",
    "- **get_all_tools** - Assembles the complete toolkit (search + MCP + reflection)\n",
    "- **get_today_str** - Provides current date context for research\n",
    "- Token limit handling utilities for graceful degradation\n",
    "\n",
    "These are defined in [`open_deep_library/utils.py`](open_deep_library/utils.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import utility functions and tools from the library\n",
    "from open_deep_library.utils import (\n",
    "    # Search tool - Lines 43-136: Tavily search with automatic summarization\n",
    "    tavily_search,\n",
    "    \n",
    "    # Reflection tool - Lines 219-244: Strategic thinking tool for ReAct pattern\n",
    "    think_tool,\n",
    "    \n",
    "    # Tool assembly - Lines 569-597: Get all configured tools\n",
    "    get_all_tools,\n",
    "    \n",
    "    # Date utility - Lines 872-879: Get formatted current date\n",
    "    get_today_str,\n",
    "    \n",
    "    # Supporting utilities for error handling\n",
    "    get_api_key_for_model,          # Lines 892-914: Get API keys from config or env\n",
    "    is_token_limit_exceeded,         # Lines 665-701: Detect token limit errors\n",
    "    get_model_token_limit,           # Lines 831-846: Look up model's token limit\n",
    "    remove_up_to_last_ai_message,    # Lines 848-866: Truncate messages for retry\n",
    "    anthropic_websearch_called,      # Lines 607-637: Detect Anthropic native search usage\n",
    "    openai_websearch_called,         # Lines 639-658: Detect OpenAI native search usage\n",
    "    get_notes_from_tool_calls,       # Lines 599-601: Extract notes from tool messages\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ❓ Question 2:  \n",
    "\n",
    "What are the advantages and disadvantages of importing these components instead of including them in the notebook?\n",
    "\n",
    "##### ✅ Answer:\n",
    "This approach provides many benefits like:\n",
    " * better maintainability: makes it easy to read and change.\n",
    " * code reuse: classes and functions defined this way can be reused in different Notebooks without code duplication.\n",
    " * code carity: the notebook is more clear and organized, easy to read and to follow along.\n",
    "\n",
    "I can't see any disvatages from using this code importing approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Configuration System\n",
    "\n",
    "The configuration system controls:\n",
    "\n",
    "### Research Behavior\n",
    "- **allow_clarification** - Whether to ask clarifying questions before research\n",
    "- **max_concurrent_research_units** - How many parallel researchers can run (default: 5)\n",
    "- **max_researcher_iterations** - How many times supervisor can delegate research (default: 6)\n",
    "- **max_react_tool_calls** - Tool call limit per researcher (default: 10)\n",
    "\n",
    "### Model Configuration\n",
    "- **research_model** - Model for research and supervision (we'll use Anthropic)\n",
    "- **compression_model** - Model for synthesizing findings\n",
    "- **final_report_model** - Model for writing the final report\n",
    "- **summarization_model** - Model for summarizing web search results\n",
    "\n",
    "### Search Configuration\n",
    "- **search_api** - Which search API to use (ANTHROPIC, TAVILY, or NONE)\n",
    "- **max_content_length** - Character limit before summarization\n",
    "\n",
    "Defined in [`open_deep_library/configuration.py`](open_deep_library/configuration.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import configuration from the library\n",
    "from open_deep_library.configuration import (\n",
    "    Configuration,    # Lines 38-247: Main configuration class with all settings\n",
    "    SearchAPI,        # Lines 11-17: Enum for search API options (ANTHROPIC, TAVILY, NONE)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Prompt Templates\n",
    "\n",
    "The system uses carefully engineered prompts for each phase:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "**clarify_with_user_instructions** - Analyzes if the research scope is clear or needs clarification\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "**transform_messages_into_research_topic_prompt** - Converts user messages into a detailed research brief\n",
    "\n",
    "### Phase 3: Supervisor\n",
    "**lead_researcher_prompt** - System prompt for the supervisor that manages delegation strategy\n",
    "\n",
    "### Phase 4: Researcher\n",
    "**research_system_prompt** - System prompt for individual researchers conducting focused research\n",
    "\n",
    "### Phase 5: Compression\n",
    "**compress_research_system_prompt** - Prompt for synthesizing research findings without losing information\n",
    "\n",
    "### Phase 6: Final Report\n",
    "**final_report_generation_prompt** - Comprehensive prompt for writing the final report\n",
    "\n",
    "All prompts are defined in [`open_deep_library/prompts.py`](open_deep_library/prompts.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import prompt templates from the library\n",
    "from open_deep_library.prompts import (\n",
    "    clarify_with_user_instructions,                    # Lines 3-41: Ask clarifying questions\n",
    "    transform_messages_into_research_topic_prompt,     # Lines 44-77: Generate research brief\n",
    "    lead_researcher_prompt,                            # Lines 79-136: Supervisor system prompt\n",
    "    research_system_prompt,                            # Lines 138-183: Researcher system prompt\n",
    "    compress_research_system_prompt,                   # Lines 186-222: Research compression prompt\n",
    "    final_report_generation_prompt,                    # Lines 228-308: Final report generation\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Node Functions - The Building Blocks\n",
    "\n",
    "Now let's look at the node functions that make up our graph. We'll import them from the library and understand what each does.\n",
    "\n",
    "### The Complete Research Workflow\n",
    "\n",
    "The workflow consists of 8 key nodes organized into 3 subgraphs:\n",
    "\n",
    "1. **Main Graph Nodes:**\n",
    "   - `clarify_with_user` - Entry point that checks if clarification is needed\n",
    "   - `write_research_brief` - Transforms user input into structured research brief\n",
    "   - `final_report_generation` - Synthesizes all research into final report\n",
    "\n",
    "2. **Supervisor Subgraph Nodes:**\n",
    "   - `supervisor` - Lead researcher that plans and delegates\n",
    "   - `supervisor_tools` - Executes supervisor's tool calls (delegation, reflection)\n",
    "\n",
    "3. **Researcher Subgraph Nodes:**\n",
    "   - `researcher` - Individual researcher conducting focused research\n",
    "   - `researcher_tools` - Executes researcher's tool calls (search, reflection)\n",
    "   - `compress_research` - Synthesizes researcher's findings\n",
    "\n",
    "All nodes are defined in [`open_deep_library/deep_researcher.py`](open_deep_library/deep_researcher.py)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 1: clarify_with_user\n",
    "\n",
    "**Purpose:** Analyzes user messages and asks clarifying questions if the research scope is unclear.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check if clarification is enabled in configuration\n",
    "2. Use structured output to analyze if clarification is needed\n",
    "3. If needed, end with a clarifying question for the user\n",
    "4. If not needed, proceed to research brief with verification message\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 60-115](open_deep_library/deep_researcher.py#L60-L115)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the clarify_with_user node\n",
    "from open_deep_library.deep_researcher import clarify_with_user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 2: write_research_brief\n",
    "\n",
    "**Purpose:** Transforms user messages into a structured research brief for the supervisor.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Use structured output to generate detailed research brief from messages\n",
    "2. Initialize supervisor with system prompt and research brief\n",
    "3. Set up supervisor messages with proper context\n",
    "\n",
    "**Why this matters:** A well-structured research brief helps the supervisor make better delegation decisions.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 118-175](open_deep_library/deep_researcher.py#L118-L175)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the write_research_brief node\n",
    "from open_deep_library.deep_researcher import write_research_brief"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 3: supervisor\n",
    "\n",
    "**Purpose:** Lead research supervisor that plans research strategy and delegates to sub-researchers.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure model with three tools:\n",
    "   - `ConductResearch` - Delegate research to a sub-agent\n",
    "   - `ResearchComplete` - Signal that research is done\n",
    "   - `think_tool` - Strategic reflection before decisions\n",
    "2. Generate response based on current context\n",
    "3. Increment research iteration count\n",
    "4. Proceed to tool execution\n",
    "\n",
    "**Decision Making:** The supervisor uses `think_tool` to reflect before delegating research, ensuring thoughtful decomposition of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 178-223](open_deep_library/deep_researcher.py#L178-L223)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor node (from supervisor subgraph)\n",
    "from open_deep_library.deep_researcher import supervisor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 4: supervisor_tools\n",
    "\n",
    "**Purpose:** Executes the supervisor's tool calls, including strategic thinking and research delegation.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check exit conditions:\n",
    "   - Exceeded maximum iterations\n",
    "   - No tool calls made\n",
    "   - `ResearchComplete` called\n",
    "2. Process `think_tool` calls for strategic reflection\n",
    "3. Execute `ConductResearch` calls in parallel:\n",
    "   - Spawn researcher subgraphs for each delegation\n",
    "   - Limit to `max_concurrent_research_units` (default: 5)\n",
    "   - Gather all results asynchronously\n",
    "4. Aggregate findings and return to supervisor\n",
    "\n",
    "**Parallel Execution:** This is where the magic happens - multiple researchers work simultaneously on different aspects of the research question.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 225-349](open_deep_library/deep_researcher.py#L225-L349)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the supervisor_tools node\n",
    "from open_deep_library.deep_researcher import supervisor_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 5: researcher\n",
    "\n",
    "**Purpose:** Individual researcher that conducts focused research on a specific topic.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Load all available tools (search, MCP, reflection)\n",
    "2. Configure model with tools and researcher system prompt\n",
    "3. Generate response with tool calls\n",
    "4. Increment tool call iteration count\n",
    "\n",
    "**ReAct Pattern:** Researchers use `think_tool` to reflect after each search, deciding whether to continue or provide their answer.\n",
    "\n",
    "**Available Tools:**\n",
    "- Search tools (Tavily or Anthropic native search)\n",
    "- `think_tool` for strategic reflection\n",
    "- `ResearchComplete` to signal completion\n",
    "- MCP tools (if configured)\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 365-424](open_deep_library/deep_researcher.py#L365-L424)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher node (from researcher subgraph)\n",
    "from open_deep_library.deep_researcher import researcher"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 6: researcher_tools\n",
    "\n",
    "**Purpose:** Executes the researcher's tool calls, including searches and strategic reflection.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Check early exit conditions (no tool calls, native search used)\n",
    "2. Execute all tool calls in parallel:\n",
    "   - Search tools fetch and summarize web content\n",
    "   - `think_tool` records strategic reflections\n",
    "   - MCP tools execute external integrations\n",
    "3. Check late exit conditions:\n",
    "   - Exceeded `max_react_tool_calls` (default: 10)\n",
    "   - `ResearchComplete` called\n",
    "4. Continue research loop or proceed to compression\n",
    "\n",
    "**Error Handling:** Safely handles tool execution errors and continues with available results.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 435-509](open_deep_library/deep_researcher.py#L435-L509)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the researcher_tools node\n",
    "from open_deep_library.deep_researcher import researcher_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 7: compress_research\n",
    "\n",
    "**Purpose:** Compresses and synthesizes research findings into a concise, structured summary.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Configure compression model\n",
    "2. Add compression instruction to messages\n",
    "3. Attempt compression with retry logic:\n",
    "   - If token limit exceeded, remove older messages\n",
    "   - Retry up to 3 times\n",
    "4. Extract raw notes from tool and AI messages\n",
    "5. Return compressed research and raw notes\n",
    "\n",
    "**Why Compression?** Researchers may accumulate lots of tool outputs and reflections. Compression ensures:\n",
    "- All important information is preserved\n",
    "- Redundant information is deduplicated\n",
    "- Content stays within token limits for the final report\n",
    "\n",
    "**Token Limit Handling:** Gracefully handles token limit errors by progressively truncating messages.\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 511-585](open_deep_library/deep_researcher.py#L511-L585)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the compress_research node\n",
    "from open_deep_library.deep_researcher import compress_research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Node 8: final_report_generation\n",
    "\n",
    "**Purpose:** Generates the final comprehensive research report from all collected findings.\n",
    "\n",
    "**Key Steps:**\n",
    "1. Extract all notes from completed research\n",
    "2. Configure final report model\n",
    "3. Attempt report generation with retry logic:\n",
    "   - If token limit exceeded, truncate findings by 10%\n",
    "   - Retry up to 3 times\n",
    "4. Return final report or error message\n",
    "\n",
    "**Token Limit Strategy:**\n",
    "- First retry: Use model's token limit × 4 as character limit\n",
    "- Subsequent retries: Reduce by 10% each time\n",
    "- Graceful degradation with helpful error messages\n",
    "\n",
    "**Report Quality:** The prompt guides the model to create well-structured reports with:\n",
    "- Proper headings and sections\n",
    "- Inline citations\n",
    "- Comprehensive coverage of all findings\n",
    "- Sources section at the end\n",
    "\n",
    "**Implementation:** [`open_deep_library/deep_researcher.py` lines 607-697](open_deep_library/deep_researcher.py#L607-L697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the final_report_generation node\n",
    "from open_deep_library.deep_researcher import final_report_generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 6: Graph Construction - Putting It All Together\n",
    "\n",
    "The system is organized into three interconnected graphs:\n",
    "\n",
    "### 1. Researcher Subgraph (Bottom Level)\n",
    "Handles individual focused research on a specific topic:\n",
    "```\n",
    "START → researcher → researcher_tools → compress_research → END\n",
    "               ↑            ↓\n",
    "               └────────────┘ (loops until max iterations or ResearchComplete)\n",
    "```\n",
    "\n",
    "### 2. Supervisor Subgraph (Middle Level)\n",
    "Manages research delegation and coordination:\n",
    "```\n",
    "START → supervisor → supervisor_tools → END\n",
    "            ↑              ↓\n",
    "            └──────────────┘ (loops until max iterations or ResearchComplete)\n",
    "            \n",
    "supervisor_tools spawns multiple researcher_subgraphs in parallel\n",
    "```\n",
    "\n",
    "### 3. Main Deep Researcher Graph (Top Level)\n",
    "Orchestrates the complete research workflow:\n",
    "```\n",
    "START → clarify_with_user → write_research_brief → research_supervisor → final_report_generation → END\n",
    "                 ↓                                       (supervisor_subgraph)\n",
    "               (may end early if clarification needed)\n",
    "```\n",
    "\n",
    "Let's import the compiled graphs from the library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pre-compiled graphs from the library\n",
    "from open_deep_library.deep_researcher import (\n",
    "    # Bottom level: Individual researcher workflow\n",
    "    researcher_subgraph,    # Lines 588-605: researcher → researcher_tools → compress_research\n",
    "    \n",
    "    # Middle level: Supervisor coordination\n",
    "    supervisor_subgraph,    # Lines 351-363: supervisor → supervisor_tools (spawns researchers)\n",
    "    \n",
    "    # Top level: Complete research workflow\n",
    "    deep_researcher,        # Lines 699-719: Main graph with all phases\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why This Architecture?\n",
    "\n",
    "### Advantages of Supervisor-Researcher Delegation\n",
    "\n",
    "1. **Dynamic Task Decomposition**\n",
    "   - Unlike section-based approaches with predefined structure, the supervisor can break down research based on the actual question\n",
    "   - Adapts to different types of research (comparisons, lists, deep dives, etc.)\n",
    "\n",
    "2. **Parallel Execution**\n",
    "   - Multiple researchers work simultaneously on different aspects\n",
    "   - Much faster than sequential section processing\n",
    "   - Configurable parallelism (1-20 concurrent researchers)\n",
    "\n",
    "3. **ReAct Pattern for Quality**\n",
    "   - Researchers use `think_tool` to reflect after each search\n",
    "   - Prevents excessive searching and improves search quality\n",
    "   - Natural stopping conditions based on information sufficiency\n",
    "\n",
    "4. **Flexible Tool Integration**\n",
    "   - Easy to add MCP tools for specialized research\n",
    "   - Supports multiple search APIs (Anthropic, Tavily)\n",
    "   - Each researcher can use different tool combinations\n",
    "\n",
    "5. **Graceful Token Limit Handling**\n",
    "   - Compression prevents token overflow\n",
    "   - Progressive truncation in final report generation\n",
    "   - Research can scale to arbitrary depths\n",
    "\n",
    "### Trade-offs\n",
    "\n",
    "- **Complexity:** More moving parts than section-based approach\n",
    "- **Cost:** Parallel researchers use more tokens (but faster)\n",
    "- **Unpredictability:** Research structure emerges dynamically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 7: Running the Deep Researcher\n",
    "\n",
    "Now let's see the system in action! We'll use it to analyze a PDF document about how people use AI.\n",
    "\n",
    "### Setup\n",
    "\n",
    "We need to:\n",
    "1. Load the PDF document\n",
    "2. Configure the execution with Anthropic settings\n",
    "3. Run the research workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded PDF with 112460 characters\n",
      "First 500 characters:\n",
      "NBER WORKING PAPER SERIES\n",
      "HOW PEOPLE USE CHATGPT\n",
      "Aaron Chatterji\n",
      "Thomas Cunningham\n",
      "David J. Deming\n",
      "Zoe Hitzig\n",
      "Christopher Ong\n",
      "Carl Yan Shan\n",
      "Kevin Wadman\n",
      "Working Paper 34255\n",
      "http://www.nber.org/papers/w34255\n",
      "NATIONAL BUREAU OF ECONOMIC RESEARCH\n",
      "1050 Massachusetts Avenue\n",
      "Cambridge, MA 02138\n",
      "September 2025\n",
      "We acknowledge help and comments from Joshua Achiam, Hemanth Asirvatham, Ryan \n",
      "Beiermeister,  Rachel Brown, Cassandra Duchan Solis, Jason Kwon, Elliott Mokski, Kevin Rao, \n",
      "Harrison Satcher,  Gawe...\n"
     ]
    }
   ],
   "source": [
    "# Load the PDF document\n",
    "from pathlib import Path\n",
    "import PyPDF2\n",
    "\n",
    "def load_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"Load and extract text from PDF.\"\"\"\n",
    "    pdf_text = \"\"\n",
    "    with open(pdf_path, 'rb') as file:\n",
    "        pdf_reader = PyPDF2.PdfReader(file)\n",
    "        for page in pdf_reader.pages:\n",
    "            pdf_text += page.extract_text() + \"\\n\\n\"\n",
    "    return pdf_text\n",
    "\n",
    "# Load the PDF about how people use AI\n",
    "pdf_path = \"data/howpeopleuseai.pdf\"\n",
    "pdf_content = load_pdf(pdf_path)\n",
    "\n",
    "print(f\"Loaded PDF with {len(pdf_content)} characters\")\n",
    "print(f\"First 500 characters:\\n{pdf_content[:500]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Graph ready for execution\n",
      "  (Note: The graph is pre-compiled from the library)\n"
     ]
    }
   ],
   "source": [
    "# Set up the graph with Anthropic configuration\n",
    "from IPython.display import Markdown, display\n",
    "import uuid\n",
    "\n",
    "# Note: deep_researcher is already compiled from the library\n",
    "# For this demo, we'll use it directly without additional checkpointing\n",
    "graph = deep_researcher\n",
    "\n",
    "print(\"✓ Graph ready for execution\")\n",
    "print(\"  (Note: The graph is pre-compiled from the library)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration for Anthropic\n",
    "\n",
    "We'll configure the system to use:\n",
    "- **Claude Sonnet 4** for all research, supervision, and report generation\n",
    "- **Tavily** for web search (you can also use Anthropic's native search)\n",
    "- **Moderate parallelism** (3 concurrent researchers)\n",
    "- **Clarification enabled** (will ask if research scope is unclear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Configuration ready\n",
      "  - Research Model: Claude Sonnet 4\n",
      "  - Max Concurrent Researchers: 3\n",
      "  - Max Iterations: 4\n",
      "  - Search API: Tavily\n"
     ]
    }
   ],
   "source": [
    "# Configure for Anthropic with moderate settings\n",
    "config = {\n",
    "    \"configurable\": {\n",
    "        # Model configuration - using Claude Sonnet 4 for everything\n",
    "        \"research_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"research_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"compression_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"compression_model_max_tokens\": 8192,\n",
    "        \n",
    "        \"final_report_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"final_report_model_max_tokens\": 10000,\n",
    "        \n",
    "        \"summarization_model\": \"anthropic:claude-sonnet-4-20250514\",\n",
    "        \"summarization_model_max_tokens\": 8192,\n",
    "        \n",
    "        # Research behavior\n",
    "        \"allow_clarification\": True,\n",
    "        \"max_concurrent_research_units\": 1,  # 1 parallel researchers\n",
    "        \"max_researcher_iterations\": 2,      # Supervisor can delegate up to 2 times\n",
    "        \"max_react_tool_calls\": 3,           # Each researcher can make up to 3 tool calls\n",
    "        \n",
    "        # Search configuration\n",
    "        \"search_api\": \"tavily\",  # Using Tavily for web search\n",
    "        \"max_content_length\": 50000,\n",
    "        \n",
    "        # Thread ID for this conversation\n",
    "        \"thread_id\": str(uuid.uuid4())\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"✓ Configuration ready\")\n",
    "print(f\"  - Research Model: Claude Sonnet 4\")\n",
    "print(f\"  - Max Concurrent Researchers: 3\")\n",
    "print(f\"  - Max Iterations: 4\")\n",
    "print(f\"  - Search API: Tavily\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Execute the Research\n",
    "\n",
    "Now let's run the research! We'll ask the system to analyze the PDF and provide insights about how people use AI.\n",
    "\n",
    "The workflow will:\n",
    "1. **Clarify** - Check if the request is clear (may skip if obvious)\n",
    "2. **Research Brief** - Transform our request into a structured brief\n",
    "3. **Supervisor** - Plan research strategy and delegate to researchers\n",
    "4. **Parallel Research** - Multiple researchers gather information simultaneously\n",
    "5. **Compression** - Each researcher synthesizes their findings\n",
    "6. **Final Report** - All findings combined into comprehensive report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting research workflow...\n",
      "\n",
      "\n",
      "============================================================\n",
      "Node: clarify_with_user\n",
      "============================================================\n",
      "\n",
      "I have sufficient information to proceed with your analysis. You've provided a comprehensive PDF document (NBER Working Paper) titled \"How People Use ChatGPT\" and requested specific insights about:\n",
      "\n",
      "1. Main findings about AI usage patterns\n",
      "2. Most common use cases  \n",
      "3. Trends and patterns from the data\n",
      "4. Startup opportunities for practical AI applications\n",
      "\n",
      "The document contains detailed research data about ChatGPT usage from November 2022 through July 2025, including user demographics, work vs. non-work usage patterns, conversation topics, and growth statistics. I will now analyze this document thoroughly to provide you with actionable insights across all four areas you've requested.\n",
      "\n",
      "============================================================\n",
      "Node: write_research_brief\n",
      "============================================================\n",
      "\n",
      "Research Brief Generated:\n",
      "I need a comprehensive analysis of the NBER Working Paper \"How People Use ChatGPT\" (Working Paper No. 34255, September 2025) by Aaron Chatterji et al. Please provide detailed insights on: (1) The main findings about how people are using AI, specifically focusing on the growth patterns from November 2022 to July 2025, the demographic shifts in adoption, the evolution from 53% to 70% non-work usage, and the geographic distribution across income levels; (2) The most common use cases, particularly a...\n",
      "\n",
      "============================================================\n",
      "Node: research_supervisor\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "Node: final_report_generation\n",
      "============================================================\n",
      "\n",
      "============================================================\n",
      "FINAL REPORT GENERATED\n",
      "============================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "# Comprehensive Analysis of NBER Working Paper \"How People Use ChatGPT\"\n",
       "\n",
       "## Main Findings About AI Usage Patterns\n",
       "\n",
       "The NBER Working Paper No. 34255 \"How People Use ChatGPT\" represents the first comprehensive academic study analyzing ChatGPT usage patterns from its November 2022 launch through July 2025. The research reveals unprecedented adoption velocity and significant demographic shifts that fundamentally challenge assumptions about AI usage patterns.\n",
       "\n",
       "### Growth Trajectory from November 2022 to July 2025\n",
       "\n",
       "ChatGPT's growth trajectory represents an unprecedented rate of technology adoption in human history. By July 2025, the platform had been adopted by approximately 10% of the world's adult population, representing roughly 700 million weekly active users sending 18 billion messages per week [1][2]. The platform reached critical milestones at extraordinary speed: 1 million users within just 5 days of launch in December 2022, 100 million weekly active users by November 2023, and weekly active users doubling every 7-8 months thereafter [7].\n",
       "\n",
       "The message volume statistics are particularly striking. By June 2025, users were sending more than 2.6 billion messages daily, equivalent to over 30,000 messages per second [7]. This represents a 5.8x increase in message volume within just the last year of the study period [7]. To contextualize this growth, the researchers note that ChatGPT reached 1 billion messages in December 2024, less than two years after release, while Google Search took eight years to reach 1 billion daily searches after its 1999 public launch [7].\n",
       "\n",
       "### Demographic Shifts in Adoption\n",
       "\n",
       "The study documents a dramatic transformation in user demographics over the study period. Early adopters were overwhelmingly male, with over 80% male users at launch [7]. However, this gender gap has essentially closed by July 2025, with 52% of active users having typically female names, representing a shift from just 37% feminine names in January 2024 [4][7]. This rapid demographic balancing suggests that initial barriers to female adoption were overcome as the technology matured and use cases diversified.\n",
       "\n",
       "Age distribution shows that nearly half of all adult messages come from users under 26, indicating strong adoption among younger demographics [2][6]. The user base has become increasingly representative of the global population rather than reflecting the typical early-adopter demographics of previous technology launches.\n",
       "\n",
       "### Evolution from 53% to 70% Non-Work Usage\n",
       "\n",
       "One of the most significant findings challenges the prevailing narrative about AI as primarily a workplace productivity tool. The research documents that non-work-related messages have grown from 53% in mid-2024 to over 70% by mid-2025, while work-related usage has remained relatively stable in absolute terms but declined as a percentage of total usage [1][2][3][4]. This shift represents both faster growth in non-work applications and changing usage patterns within existing user cohorts rather than simply compositional changes from new users.\n",
       "\n",
       "The implications of this finding are profound for economic analysis. While most AI research has focused on workplace productivity impacts, the study suggests that consumer surplus and home production effects may be equally or more significant. This aligns with research by Collis and Brynjolfsson (2025), who estimate consumer surplus of at least $97 billion in 2024 alone in the US from generative AI [1].\n",
       "\n",
       "### Geographic Distribution Across Income Levels\n",
       "\n",
       "The study reveals surprising patterns in global adoption that contradict expectations about digital divide effects. Higher growth rates are consistently observed in lower-income countries, suggesting a democratization effect rather than exacerbation of digital inequality [1][3][4]. By May 2025, adoption growth rates in the lowest income countries were over 4 times those in the highest income countries [4][7].\n",
       "\n",
       "The convergence in usage rates across income levels is particularly striking. Brazil, South Korea, and the United States show similar ChatGPT usage rates despite having GDP per capita of $10,000, $34,000, and $86,000 respectively [7]. Countries at the 50th versus 90th percentile of GDP per capita now demonstrate similar usage rates, with the fastest growth occurring in middle-income countries [7].\n",
       "\n",
       "## Most Common Use Cases and Their Analysis\n",
       "\n",
       "The researchers employed an automated classification system using privacy-preserving methods to categorize conversations without human review of content. This methodology revealed that nearly 80% of all ChatGPT conversations fall into three dominant categories: Practical Guidance, Seeking Information, and Writing [1][2][3][4][6].\n",
       "\n",
       "### Practical Guidance (29% of All Conversations)\n",
       "\n",
       "Practical Guidance represents the largest single category at approximately 29% of all conversations [2][6]. This category encompasses personalized advice across diverse domains including workout planning, tutoring, teaching, and decision-making support. The tutoring subcategory alone represents 10% of all messages, indicating that educational applications constitute a major use case [2][6].\n",
       "\n",
       "The prevalence of Practical Guidance reflects ChatGPT's unique ability to provide personalized, contextual advice that adapts to individual circumstances. Unlike traditional information retrieval systems, this category leverages the conversational interface to deliver tailored recommendations and step-by-step guidance. The educational component is particularly significant, as it suggests ChatGPT is functioning as a democratized tutoring resource accessible to users regardless of geographic location or economic circumstances.\n",
       "\n",
       "### Seeking Information (24% of All Conversations)\n",
       "\n",
       "The Seeking Information category has grown substantially from 14% to 24% over the study period [6][8], functioning essentially as a conversational search engine. This category includes searches for information about people, current events, products, and recipes, positioning ChatGPT as a direct substitute for traditional web search in many contexts.\n",
       "\n",
       "The growth trajectory of this category is particularly notable because it represents a shift in how users approach information retrieval. Rather than keyword-based searches that require users to formulate precise queries, ChatGPT allows for natural language information requests that can be refined through dialogue. This conversational approach to information seeking may explain the category's rapid growth as users discover its advantages over traditional search engines.\n",
       "\n",
       "### Writing (24% of All Conversations, 40% of Work-Related Messages)\n",
       "\n",
       "Writing represents 24% of overall conversations but dominates work-related tasks, accounting for 40% of work-related messages as of June 2025 [2]. This category includes automated production of emails, documents, and communications, as well as editing, critiquing, summarizing, and translating user-provided text.\n",
       "\n",
       "Significantly, about two-thirds of all Writing messages ask ChatGPT to modify, edit, or translate existing user text rather than creating entirely new content from scratch [2][8]. This finding suggests that users primarily view ChatGPT as an enhancement tool for their own writing rather than a replacement for human creativity. The emphasis on modification and editing indicates that users maintain agency over content creation while leveraging AI for refinement and improvement.\n",
       "\n",
       "Despite its prominence in work contexts, the Writing category has declined from 36% to 24% of overall usage [8], reflecting the faster growth of other categories rather than an absolute decline in writing applications.\n",
       "\n",
       "## Trends and Patterns Emerging from the Data\n",
       "\n",
       "### Shift in Work vs Non-Work Usage Patterns\n",
       "\n",
       "The most significant trend documented in the study is the accelerating growth of non-work applications relative to workplace usage. This shift represents a fundamental change in how AI technology is being integrated into daily life, moving beyond the initial focus on workplace productivity to encompass broader life enhancement applications.\n",
       "\n",
       "The researchers found that this trend reflects changing usage patterns within existing user cohorts rather than compositional effects from new users with different preferences. All user cohorts, including early adopters from Q1 2023, showed increased usage beginning in early 2025, with early adopters sending 40% more messages per day by July 2025 compared to two years earlier [7]. This suggests that ChatGPT has become substantially more capable and user-friendly, leading existing users to expand their usage into non-work domains.\n",
       "\n",
       "### Changes in User Demographics Over Time\n",
       "\n",
       "Beyond the gender convergence already discussed, the study reveals broader demographic trends that challenge assumptions about AI adoption patterns. The rapid expansion beyond traditional early-adopter demographics suggests that ChatGPT has achieved mainstream acceptance across diverse population segments.\n",
       "\n",
       "The age distribution, with nearly half of adult messages coming from users under 26, indicates strong generational adoption that may have long-term implications for how AI tools are integrated into various life domains. This younger user base is likely experimenting with novel applications and use cases that older users might not explore, potentially driving innovation in non-work applications.\n",
       "\n",
       "### Differences in Usage by Education and Occupation Levels\n",
       "\n",
       "Work usage correlates strongly with higher education levels and professional occupations [1][3][6]. Users in knowledge-intensive jobs are more likely to use ChatGPT for work-related tasks, particularly writing and document creation. This finding suggests that current AI tools are most readily applicable to jobs involving significant text-based work and decision-making.\n",
       "\n",
       "However, the growth in non-work usage across all demographic segments indicates that ChatGPT's value proposition extends well beyond professional applications. The democratization effect observed across income levels suggests that educational and occupational barriers to adoption are diminishing as the technology matures and use cases diversify.\n",
       "\n",
       "### Evolution of Conversation Topics and User Intent\n",
       "\n",
       "The researchers developed a user intent classification system revealing that 49% of messages are \"Asking\" (seeking information or advice), 40% are \"Doing\" (performing specific tasks), and 11% are \"Expressing\" (social or emotional content) [2]. The prominence of \"Asking\" behaviors indicates that users primarily value ChatGPT as an advisory tool rather than merely for task automation.\n",
       "\n",
       "For work-related messages specifically, \"Doing\" comprises 56% of usage, with most focused on writing tasks [2]. This distinction between work and non-work intent patterns suggests that professional usage emphasizes task completion while personal usage emphasizes consultation and advice-seeking.\n",
       "\n",
       "Surprising findings include the relatively small share of certain anticipated use cases. Only 4.2% of messages are programming-related [2][6], despite widespread assumptions about AI coding applications. Similarly, just 1.9% involve relationships or personal reflection, and only 0.4% involve games or role-play [8], contradicting narratives about AI companions or entertainment applications.\n",
       "\n",
       "## Startup Opportunities for Practical AI Applications\n",
       "\n",
       "The comprehensive usage data from this study reveals significant market opportunities for startups developing practical AI applications. The patterns suggest both underserved market segments and emerging needs that current generative platforms may not fully address.\n",
       "\n",
       "### Gaps in Current Usage Patterns\n",
       "\n",
       "Despite ChatGPT's broad adoption, several usage categories remain surprisingly underrepresented, suggesting market opportunities for specialized solutions. Programming represents only 4.2% of overall usage [2][6], indicating potential for developer-focused AI tools that provide more specialized coding assistance than general-purpose chatbots. The relatively low share of programming usage, compared to 33% of work-related Claude conversations, suggests room for more targeted development tools.\n",
       "\n",
       "The small percentage of social and emotional content (11% \"Expressing\" category, 1.9% relationships/personal reflection) indicates opportunities for AI applications specifically designed for mental health, relationship advice, or emotional support. Current general-purpose tools may not provide the specialized approaches needed for these sensitive applications.\n",
       "\n",
       "Educational applications, while representing 10% of overall usage through tutoring, could be significantly expanded with purpose-built educational AI tools. The strong adoption of Practical Guidance suggests demand for more sophisticated, curriculum-aligned educational applications that go beyond basic tutoring.\n",
       "\n",
       "### Underserved Use Cases and Market Niches\n",
       "\n",
       "The dominance of three categories (Practical Guidance, Seeking Information, Writing) accounting for nearly 80% of usage suggests that the remaining 20% represents numerous smaller use cases that might benefit from specialized tools. These could include:\n",
       "\n",
       "**Specialized Professional Tools**: While writing dominates work usage at 40% of work-related messages, other professional functions may be underserved. Legal document analysis, medical decision support, financial planning, and technical consultation represent potential niches for industry-specific AI applications.\n",
       "\n",
       "**Enhanced Decision Support Systems**: The research emphasizes ChatGPT's primary value as a decision-support tool, particularly for knowledge-intensive jobs [1][6]. Startups could develop more sophisticated decision support systems for specific industries or decision types, incorporating domain-specific data, regulatory requirements, or decision frameworks.\n",
       "\n",
       "**Localized and Cultural Applications**: The global adoption patterns suggest opportunities for AI tools that better serve specific cultural contexts, languages, or regional needs. While ChatGPT shows broad international adoption, specialized tools addressing local customs, languages, or regulatory environments could capture specific market segments.\n",
       "\n",
       "### Potential Market Opportunities Based on Demographic and Usage Data\n",
       "\n",
       "The demographic evolution documented in the study reveals several market opportunities:\n",
       "\n",
       "**Elderly User Applications**: With nearly half of users under 26, there appears to be significant opportunity for AI tools specifically designed for older adults. These might emphasize ease of use, larger interfaces, voice interaction, or content relevant to retirement, health management, or family coordination.\n",
       "\n",
       "**Educational Technology for Developing Markets**: The higher growth rates in lower-income countries suggest strong demand for educational AI tools in emerging markets. Startups could develop specialized educational applications optimized for mobile devices, low-bandwidth environments, or local educational systems.\n",
       "\n",
       "**Small Business Productivity Tools**: The correlation between education/occupation levels and work usage suggests that less-educated workers or small business owners may be underserved by current AI tools. Simplified, task-specific AI applications for trades, retail, or service businesses could address this gap.\n",
       "\n",
       "**Integration and Workflow Tools**: The growth in usage intensity among existing users suggests demand for AI tools that integrate more seamlessly with existing workflows and applications rather than requiring separate interactions with general-purpose chatbots.\n",
       "\n",
       "### Areas of Unmet Demand and Emerging Needs\n",
       "\n",
       "The study data suggests several areas where current AI tools may not fully meet user needs:\n",
       "\n",
       "**Advanced Writing Collaboration**: While two-thirds of writing tasks involve modifying existing text rather than creating new content, current tools may not optimize for collaborative writing workflows. Specialized writing tools that better integrate with existing document systems or provide more sophisticated editing capabilities could serve this need.\n",
       "\n",
       "**Complex Decision Support**: The prominence of Practical Guidance and the emphasis on decision support suggest demand for more sophisticated advisory tools that can handle complex, multi-factor decisions with higher stakes than general chatbots typically address.\n",
       "\n",
       "**Privacy-Focused Applications**: The study's emphasis on privacy-preserving analysis methods suggests user concern about data privacy. Startups developing AI tools with enhanced privacy protections, local processing, or industry-specific compliance requirements could capture privacy-conscious market segments.\n",
       "\n",
       "**Vertical Integration Opportunities**: The broad usage patterns suggest opportunities for AI tools that integrate multiple functions (information seeking, writing, guidance) within specific domains such as healthcare, finance, education, or legal services.\n",
       "\n",
       "The research methodology itself, using automated classification and privacy-preserving analysis, suggests opportunities for startups developing AI analytics tools that help organizations understand their own AI usage patterns while maintaining privacy and compliance requirements.\n",
       "\n",
       "### Sources\n",
       "\n",
       "[1] How People Use ChatGPT | NBER: https://www.nber.org/papers/w34255\n",
       "[2] [PDF] How People Use ChatGPT - National Bureau of Economic Research: https://www.nber.org/system/files/working_papers/w34255/w34255.pdf\n",
       "[3] How People Use ChatGPT - SSRN: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=5487080\n",
       "[4] How people are using ChatGPT | OpenAI: https://openai.com/index/how-people-are-using-chatgpt/\n",
       "[5] How People Actually Use ChatGPT — What 1.5M Conversations Tell Us About the Next Decade of Software: https://medium.com/@adnanmasood/how-people-actually-use-chatgpt-what-1-5m-conversations-tell-us-about-the-next-decade-of-software-ea603212b458\n",
       "[6] How People Really Use ChatGPT: Findings from NBER Research: https://techmaniacs.com/2025/09/15/how-people-really-use-chatgpt-findings-from-nber-research/\n",
       "[7] How People Use ChatGPT - by David Deming - Forked Lightning: https://forklightning.substack.com/p/how-people-use-chatgpt\n",
       "[8] What Over 2.5 Billion Daily Messages Reveal About How People Use ChatGPT: https://c3.unu.edu/blog/what-over-2-5-billion-daily-messages-reveal-about-how-people-use-chatgpt"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "Research workflow completed!\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# Create our research request with PDF context\n",
    "research_request = f\"\"\"\n",
    "I have a PDF document about how people use AI. Please analyze this document and provide insights about:\n",
    "\n",
    "1. What are the main findings about how people are using AI?\n",
    "2. What are the most common use cases?\n",
    "3. What trends or patterns emerge from the data?\n",
    "4. Which opportunities are there for practical AI applications that can be explored by a startup?\n",
    "\n",
    "Here's the PDF content:\n",
    "\n",
    "{pdf_content[:10000]}  # First 10k chars to stay within limits\n",
    "\n",
    "...[content truncated for context window]\n",
    "\"\"\"\n",
    "\n",
    "# Execute the graph\n",
    "async def run_research():\n",
    "    \"\"\"Run the research workflow and display results.\"\"\"\n",
    "    print(\"Starting research workflow...\\n\")\n",
    "    \n",
    "    async for event in graph.astream(\n",
    "        {\"messages\": [{\"role\": \"user\", \"content\": research_request}]},\n",
    "        config,\n",
    "        stream_mode=\"updates\"\n",
    "    ):\n",
    "        # Display each step\n",
    "        for node_name, node_output in event.items():\n",
    "            print(f\"\\n{'='*60}\")\n",
    "            print(f\"Node: {node_name}\")\n",
    "            print(f\"{'='*60}\")\n",
    "            \n",
    "            if node_name == \"clarify_with_user\":\n",
    "                if \"messages\" in node_output:\n",
    "                    last_msg = node_output[\"messages\"][-1]\n",
    "                    print(f\"\\n{last_msg.content}\")\n",
    "            \n",
    "            elif node_name == \"write_research_brief\":\n",
    "                if \"research_brief\" in node_output:\n",
    "                    print(f\"\\nResearch Brief Generated:\")\n",
    "                    print(f\"{node_output['research_brief'][:500]}...\")\n",
    "            \n",
    "            elif node_name == \"supervisor\":\n",
    "                print(f\"\\nSupervisor planning research strategy...\")\n",
    "                if \"supervisor_messages\" in node_output:\n",
    "                    last_msg = node_output[\"supervisor_messages\"][-1]\n",
    "                    if hasattr(last_msg, 'tool_calls') and last_msg.tool_calls:\n",
    "                        print(f\"Tool calls: {len(last_msg.tool_calls)}\")\n",
    "                        for tc in last_msg.tool_calls:\n",
    "                            print(f\"  - {tc['name']}\")\n",
    "            \n",
    "            elif node_name == \"supervisor_tools\":\n",
    "                print(f\"\\nExecuting supervisor's tool calls...\")\n",
    "                if \"notes\" in node_output:\n",
    "                    print(f\"Research notes collected: {len(node_output['notes'])}\")\n",
    "            \n",
    "            elif node_name == \"final_report_generation\":\n",
    "                if \"final_report\" in node_output:\n",
    "                    print(f\"\\n\" + \"=\"*60)\n",
    "                    print(\"FINAL REPORT GENERATED\")\n",
    "                    print(\"=\"*60 + \"\\n\")\n",
    "                    display(Markdown(node_output[\"final_report\"]))\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"Research workflow completed!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "# Run the research\n",
    "await run_research()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Output\n",
    "\n",
    "Let's break down what happened:\n",
    "\n",
    "### Phase 1: Clarification\n",
    "The system checked if your request was clear. Since you provided a PDF and specific questions, it likely proceeded without clarification.\n",
    "\n",
    "### Phase 2: Research Brief\n",
    "Your request was transformed into a detailed research brief that guides the supervisor's delegation strategy.\n",
    "\n",
    "### Phase 3: Supervisor Delegation\n",
    "The supervisor analyzed the brief and decided how to break down the research:\n",
    "- Used `think_tool` to plan strategy\n",
    "- Called `ConductResearch` multiple times to delegate to parallel researchers\n",
    "- Each delegation specified a focused research topic\n",
    "\n",
    "### Phase 4: Parallel Research\n",
    "Multiple researchers worked simultaneously:\n",
    "- Each researcher used web search tools to gather information\n",
    "- Used `think_tool` to reflect after each search\n",
    "- Decided when they had enough information\n",
    "- Compressed their findings into clean summaries\n",
    "\n",
    "### Phase 5: Final Report\n",
    "All research findings were synthesized into a comprehensive report with:\n",
    "- Well-structured sections\n",
    "- Inline citations\n",
    "- Sources listed at the end\n",
    "- Balanced coverage of all findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 🏗️ Activity #1: Try Different Configurations\n",
    "\n",
    "You can experiment with different settings to see how they affect the research.  You may select three or more of the following settings (or invent your own experiments) and describe the results.\n",
    "\n",
    "### Increase Parallelism\n",
    "```python\n",
    "\"max_concurrent_research_units\": 10  # More researchers working simultaneously\n",
    "```\n",
    "\n",
    "### Deeper Research\n",
    "```python\n",
    "\"max_researcher_iterations\": 8   # Supervisor can delegate more times\n",
    "\"max_react_tool_calls\": 15      # Each researcher can search more\n",
    "```\n",
    "\n",
    "### Use Anthropic Native Search\n",
    "```python\n",
    "\"search_api\": \"anthropic\"  # Use Claude's built-in web search\n",
    "```\n",
    "\n",
    "### Disable Clarification\n",
    "```python\n",
    "\"allow_clarification\": False  # Skip clarification phase\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "### Architecture Benefits\n",
    "1. **Dynamic Decomposition** - Research structure emerges from the question, not predefined\n",
    "2. **Parallel Efficiency** - Multiple researchers work simultaneously\n",
    "3. **ReAct Quality** - Strategic reflection improves search decisions\n",
    "4. **Scalability** - Handles token limits gracefully through compression\n",
    "5. **Flexibility** - Easy to add new tools and capabilities\n",
    "\n",
    "### When to Use This Pattern\n",
    "- **Complex research questions** that need multi-angle investigation\n",
    "- **Comparison tasks** where parallel research on different topics is beneficial\n",
    "- **Open-ended exploration** where structure should emerge dynamically\n",
    "- **Time-sensitive research** where parallel execution speeds up results\n",
    "\n",
    "### When to Use Section-Based Instead\n",
    "- **Highly structured reports** with predefined format requirements\n",
    "- **Template-based content** where sections are always the same\n",
    "- **Sequential dependencies** where later sections depend on earlier ones\n",
    "- **Budget constraints** where token efficiency is critical"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "### Extend the System\n",
    "1. **Add MCP Tools** - Integrate specialized tools for your domain\n",
    "2. **Custom Prompts** - Modify prompts for specific research types\n",
    "3. **Different Models** - Try different Claude versions or mix models\n",
    "4. **Persistence** - Use a real database for checkpointing instead of memory\n",
    "\n",
    "### Learn More\n",
    "- [LangGraph Documentation](https://langchain-ai.github.io/langgraph/)\n",
    "- [Open Deep Research Repo](https://github.com/langchain-ai/open_deep_research)\n",
    "- [Anthropic Claude Documentation](https://docs.anthropic.com/)\n",
    "- [Tavily Search API](https://tavily.com/)\n",
    "\n",
    "### Deploy\n",
    "- Use LangGraph Cloud for production deployment\n",
    "- Add proper error handling and logging\n",
    "- Implement rate limiting and cost controls\n",
    "- Monitor research quality and costs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
